# PRODIGY_GA_01
A project demonstrating how to fine-tune OpenAI’ s GPT-2 model on a custom text dataset using Hugging Face Transformers. The model learns writing style and generates coherent, context-aware text from user prompts.
This project demonstrates how to fine-tune OpenAI’s GPT-2 model using a small, custom dataset of sentences. It uses Hugging Face’s Transformers, Datasets, and PyTorch libraries to train the model so it can generate context-aware, coherent, and creative text. The training pipeline includes data preprocessing, tokenization, model training, saving, and text generation. Ideal for learning and experimenting with custom language modeling.

